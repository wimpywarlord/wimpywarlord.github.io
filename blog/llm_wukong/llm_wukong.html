<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BLOG_TITLE - Tjay</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Display:ital,wght@0,100..900;1,100..900&display=swap"
    rel="stylesheet">
  <style>
    /* Base styles matching main site */
    body {
      font-family: "Noto Sans Display", sans-serif;
      background-color: oklch(0.269 0 0);
      color: oklch(0.985 0 0);
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }

    /* Reset link styles */
    a {
      color: inherit;
      text-decoration: none;
    }

    a:hover,
    a:focus-visible {
      text-decoration: underline;
    }

    /* Container */
    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    /* Navigation */
    .back-link {
      display: inline-flex;
      align-items: center;
      font-size: 0.9rem;
      color: oklch(0.7 0 0);
      margin-bottom: 2rem;
      transition: color 0.15s;
    }

    .back-link:hover {
      color: oklch(0.985 0 0);
    }

    /* Blog post styles */
    .blog-post {
      max-width: 700px;
      margin: 6rem auto;
    }

    .post-header {
      margin-bottom: 3rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid oklch(0.3 0 0);
    }

    .post-meta {
      display: flex;
      align-items: center;
      gap: 1rem;
      margin-bottom: 0.5rem;
    }

    .post-date {
      font-size: 0.85rem;
      color: oklch(0.6 0 0);
      font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
    }

    .post-status {
      font-size: 0.75rem;
      background: oklch(0.18 0 0 / 0.4);
      padding: 0.15rem 0.45rem;
      border-radius: 0.25rem;
      color: #ffb86c;
      font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
    }

    .post-title {
      font-size: 1.5rem;
      font-weight: 700;
      margin: 0;
      line-height: 1.2;
      color: oklch(0.985 0 0);
    }

    .post-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
    }

    .tag {
      font-size: 0.75rem;
      background: oklch(0.18 0 0 / 0.4);
      padding: 0.15rem 0.45rem;
      border-radius: 0.25rem;
      color: oklch(0.7 0 0);
      font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
      transition: background 0.15s;
    }

    .tag:hover {
      background: oklch(0.25 0 0 / 0.5);
    }

    /* Post content */
    .post-content {
      font-size: 1rem;
      line-height: 1.6;
      color: oklch(0.9 0 0);
    }

    .post-content h2 {
      font-size: 1.2rem;
      font-weight: 600;
      margin: 1.5rem 0 0.8rem 0;
      color: oklch(0.985 0 0);
    }

    .post-content h3 {
      font-size: 1rem;
      font-weight: 600;
      margin: 1.2rem 0 0.6rem 0;
      color: oklch(0.985 0 0);
    }

    .post-content p {
      margin: 0.8rem 0;
    }

    .post-content ul, .post-content ol {
      margin: 0.8rem 0;
      padding-left: 1.5rem;
    }

    .post-content li {
      margin: 0.3rem 0;
    }

    .post-content a {
      color: #ffb86c;
      text-decoration: underline;
      text-decoration-thickness: 1px;
      text-underline-offset: 2px;
    }

    .post-content a:hover {
      color: #ffa500;
    }

    .post-content code {
      font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
      background: oklch(0.18 0 0 / 0.4);
      padding: 0.1rem 0.3rem;
      border-radius: 0.2rem;
      font-size: 0.9em;
    }

    .post-content pre {
      background: oklch(0.15 0 0);
      padding: 1rem;
      border-radius: 0.5rem;
      overflow-x: auto;
      margin: 1rem 0;
      border: 1px solid oklch(0.3 0 0);
    }

    .post-content pre code {
      background: none;
      padding: 0;
      color: oklch(0.9 0 0);
    }

    .post-content blockquote {
      border-left: 4px solid #ffb86c;
      padding-left: 1rem;
      margin: 1rem 0;
      font-style: italic;
      color: oklch(0.8 0 0);
    }

    .post-content img {
      max-width: 100%;
      height: auto;
      border-radius: 0.5rem;
      margin: 1rem 0;
    }

    /* Post footer */
    .post-footer {
      margin-top: 4rem;
      padding-top: 2rem;
      border-top: 1px solid oklch(0.3 0 0);
    }

    .post-navigation {
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .nav-link {
      font-size: 0.9rem;
      color: oklch(0.7 0 0);
      transition: color 0.15s;
    }

    .nav-link:hover {
      color: oklch(0.985 0 0);
    }

    /* Responsive design */
    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }
      
      .post-title {
        font-size: 1.3rem;
      }
      
      .post-content {
        font-size: 0.9rem;
      }
      
      .post-content h2 {
        font-size: 1.1rem;
      }
      
      .post-content h3 {
        font-size: 0.95rem;
      }
      
      .post-navigation {
        flex-direction: column;
        gap: 1rem;
        align-items: flex-start;
      }
      
      .post-meta {
        flex-direction: column;
        align-items: flex-start;
        gap: 0.5rem;
      }
    }
  </style>
</head>

<body>
  <div class="container">
    <article class="blog-post">
      <header class="post-header">
        <div class="post-meta">
          <span class="post-date">10th July 2025</span>
          <!-- <span class="post-status">Published</span> -->
        </div>
        <h1 class="post-title">How to make an LLM play Black Myth: Wukong</h1>
      </header>

      <div class="post-content">
        <p>Let me give you a taste of what we are building:</p>

        <video controls style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0 0.3rem 0;">
          <source src="assets/dodge.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: oklch(0.7 0 0);">New input paradigm - controlling 3D embodied characters through natural speech.</p>

        <video controls style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0 0.3rem 0;">
          <source src="assets/smash.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: oklch(0.7 0 0);">Context awareness - control input (space-bar press for dodging) is fed into the system context - the robot responds accordingly.</p>

        <video controls style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0 0.3rem 0;">
          <source src="assets/time.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: oklch(0.7 0 0);">Spatial + Vision + Time awareness - human like immersion.</p>

        <div style="display: flex; gap: 1rem; margin: 1rem 0 0.3rem 0;">
          <img src="assets/personality_config.png" alt="Personality Configuration" style="flex: 1; max-width: 50%; border-radius: 0.5rem;">
          <img src="assets/dashboard.png" alt="Dashboard" style="flex: 1; max-width: 50%; border-radius: 0.5rem;">
        </div>

        <p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: oklch(0.7 0 0);">Personality + characteristics + speech styles can be tailored.</p>

        <p><strong>Release Information:</strong> <a href="https://www.ready.mp/" target="_blank">https://www.ready.mp/</a></p>

        <h2>Here is how we built it?</h2>

        <p>The number 1 problem you will run into while building something like this is - <strong>Latency</strong>.</p>
        <p><strong>Ideal total latency</strong> for video games: 20‚Äì60 ms (that pretty FAST!). <img src="assets/sonic-running.gif" alt="Sonic Running" style="height: 1.2em; vertical-align: middle;"> However average latency for LLMs is 6 seconds (anecdotally).</p>

        <img src="assets/latency_chart.png" alt="Latency Chart" style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0;">

        <p>No one wants to see an autistic alien use the joystick at turtle speeds üê¢.</p>

        <img src="assets/no_brain.jpg" alt="No Brain" style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0;">

        <p>Computer controlled gaming is still in its early stages, there have been a <a href="https://github.com/BAAI-Agents/Cradle" target="_blank">few</a> great attempts, but latency and LLM agency is a long way out.</p>

        <h3>Can you engineer your WAY through this?</h3>

        <p>Short answer Yes. Here is how:</p>

        <img src="assets/flowchart.png" alt="Flowchart" style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0;">

        <p>The fundamental difference in approach here is to write game scripts (pre-meditated or generated on the fly by the LLM) for controlling the agent input - LLMs never controls the agent input directly. LLM only decides what game script to run. The scripts are then injected in the game engine and executed in realtime.</p>

        <p>This approach is inspired from <a href="https://github.com/MineDojo/Voyager" target="_blank">Voyager</a> from Minecraft. This is where I see the industry heading in the short to mid term.</p>

        <p>Example of how I implemented this approach in Wukong:</p>

        <pre><code>system_prompt: |
  You are co-op playing Wukong video game with the user: 

  Here is your personality:
  {personality:}

  The user is asking you to perform an action.

  Here is the user message:
  {user_message:}

  Analyze the user message and classify it into one of these action types:
  1. combat: Combat or attacking actions
  2. spells: Casting or using spells
  3. transform: Transformation or changing form
  4. unsupported: Any action that doesn't fit the above categories

  You must respond with a JSON object containing an 'action_type' field with one of these values.
  - "combat"
  - "spells"
  - "transform"
  - "unsupported"

  The "response_text" and "response_tone" should be according to your personality.

  If the action_type is "unsupported", the response_text should be a witty response but also convey that the only actions you can perform are "combat", "spells", or "transform".

  {length_instruction:}

  Example response format:
  {"action_type": "combat", "response_text": "You swing your sword at the enemy.", "response_tone": "aggressive"}</code></pre>

        <p>You see how the LLM is only ever deciding what action to perform. It is never actually trying to perform the action in real-time using computer control. It just returns the "action-string".</p>

        <p>Action execution is handled by game-scripts that are executed inside the game engine (unreal or unity) based on the "action-string" it receives from the LLM.</p>

        <p>I know some of you are going to be like - ACKHYUALLY - that is not the LLM playing the game.</p>

        <img src="assets/actually.png" alt="Actually" style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0;">

        <p>But I would like to argue - I am using LLMs for what they are ACKHYUALLY good at - <strong>REASONING</strong>.</p>

        <p><strong>AND</strong></p>

        <p>I am using the game engine for it is ACKHYUALLY good at - <strong>CONTROL INPUT</strong>.</p>

        <p>I know some of you are thinking - it is not generalizable to other games. You will need to pre-meditate game scripts for all possible items in the "action-space" for each game (lotta manual labor).</p>

        <p>Well that is where having the LLM generate game scripts on the fly comes into play. Voyager has achieved great success at that with Minecraft.</p>

        <video controls style="max-width: 100%; border-radius: 0.5rem; margin: 1rem 0;">
          <source src="assets/voyager.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <h3>Optimizations</h3>

        <p>6 seconds is still however a lot of latency - even if the LLM is just generating the "action-string". There are other modules such as TTS/STT, Game Vision + Game Log ingestion, RAG, script injection etc that might add more latency on top of the LLM.</p>

        <p>The number 1 unlock here is to <strong>parallelize your game input ingestion pipeline</strong>. As you will notice in the above diagram, the vision and log ingestion are a non blocking operation for User action/dialogue requests.</p>

        <p>One drawback - the LLM's context can lag 10 seconds (talks about killing a monster, when you have moved on to the next stage).</p>

        <p>Another unlock is to <strong>reduce input tokens</strong>. Have the LLM summarize the vision/logs into text summary. This will reduce cost and latency for all future LLM calls.</p>

        <pre><code>system_prompt: |
  You are analyzing {NAME_OF_THE_GAME:} game.
  The description of the game is as follows: {DESCRIPTION_OF_THE_GAME:}
  The plot of the game is as follows: {PLOT_OF_THE_GAME:}
  The game context data is as follows: {GAME_CONTEXT_DATA:}

  Your task is to analyze the game vision and provide a summary of what the user is seeing.

  Here is what the user is seeing in the game:</code></pre>

        <p>Another interesting concept I have recently learned is called <a href="https://www.reddit.com/r/OpenAI/comments/1l70htm/the_23_solution_why_running_redundant_llms_is/" target="_blank">LLM Drag-Racing</a> - beware - its expensive</p>

        <p>The crux of the concept is LLM are often faster than 6 seconds latency but they are un-reliable in doing so. What if we could make LLM calls to 5 different providers and use the one with fastest response, and do this for every call. On average you will end up with much lower overall latency.</p>

        <p>Another big latency add-on is <strong>Text to Speech (TTS) and Speech to Text (STT) API calls</strong>. They each add around 3 seconds to the latency (depending on text length).</p>

        <p>The best work around here is to use <a href="https://platform.openai.com/docs/guides/realtime" target="_blank">Real Time APIs</a>.</p>

        <p>These APIs require you to establish a websocket connection directly with the server hosting these LLMs. Additionally, the input and output are streamed in audio/text chunks. So you can start relaying that audio/text chunks to the user as you receive them.</p>

        <p>Additionally, these APIs are capable of accepting audio input directly (no need to perform STT), and also audio output directly (no TTS required).</p>

        <p>The limitation with these APIs is that they are heavily censored. For example: instructing wukong to go kill the monster wolf can be flagged as policy violation.</p>

        <p>I will keep adding more insights about building agentic infrastructure as I come across them.</p>
      </div>

      <footer class="post-footer">
        <div class="post-navigation">
          <a href="../../index.html" class="nav-link">‚Üê Back to Home</a>
        </div>
      </footer>
    </article>
  </div>
</body>

</html> 